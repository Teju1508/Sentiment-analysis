{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sentiment_analysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "__NFhsLCaD3z",
        "outputId": "2a7e65b7-034f-4774-f717-23946436e4ba"
      },
      "source": [
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "import re\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqVtFz1ZcG-Z",
        "outputId": "df0377a4-a20c-484d-de82-cf4d123a07a4"
      },
      "source": [
        "train_data = pd.read_csv('https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/train.csv')\n",
        "#print(train_data)\n",
        "\n",
        "test_datas = pd.read_csv('https://raw.githubusercontent.com/dD2405/Twitter_Sentiment_Analysis/master/test.csv')\n",
        "#print(test_data)\n",
        "print(train_data['label'].loc[0] == 0)\n",
        "#train_data = train_dataset.append(test_dataset, ignore_index = True, sort = True)\n",
        "#print(train_data)\n",
        "\n",
        "# str1 = train_data['tweet'].loc[0]\n",
        "# print(str1)\n",
        "# str1 = re.sub('[@][^\\s]+|[^A-Za-z# ]+','',str1)\n",
        "# print(str1)\n",
        "\n",
        "def reg(text) :\n",
        "  \n",
        "  text = re.sub('[@][^\\s]+|[^A-Za-z# ]+',\" \",text)\n",
        "  return text\n",
        "\n",
        "#reg = np.frompyfunc(reg,1,1)\n",
        "#l = reg(train_data['tweet'].to_numpy())\n",
        "#print(l)\n",
        "train_data['new_tweet'] = np.vectorize(reg)(train_data['tweet'])\n",
        "\n",
        "\n",
        "train_data['new_tweet'] = train_data['new_tweet'].apply(lambda x : x.split())\n",
        "train_data['new_tweet'] = train_data['new_tweet'].apply(lambda x : [i for i in x if len(i)>3])\n",
        "print(train_data['new_tweet'])\n",
        "#print(type(l))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "0        [when, father, dysfunctional, selfish, drags, ...\n",
            "1        [thanks, #lyft, credit, cause, they, offer, wh...\n",
            "2                                  [bihday, your, majesty]\n",
            "3                         [#model, love, take, with, time]\n",
            "4                       [factsguide, society, #motivation]\n",
            "                               ...                        \n",
            "31957                                        [that, youuu]\n",
            "31958    [nina, turner, airwaves, trying, wrap, herself...\n",
            "31959            [listening, songs, monday, morning, work]\n",
            "31960    [#sikh, #temple, vandalised, #calgary, #wso, c...\n",
            "31961                                      [thank, follow]\n",
            "Name: new_tweet, Length: 31962, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WA5p1s27rIU",
        "outputId": "6fb8f0ce-5d91-418c-a3b4-80cda2659ff4"
      },
      "source": [
        "stemmer = PorterStemmer()\n",
        "\n",
        "train_data['new_tweet'] = train_data['new_tweet'].apply(lambda x : ' '.join([stemmer.stem(i) for i in x] ))\n",
        "print(train_data['new_tweet'])\n",
        "\n",
        "\n",
        "# lemma = WordNetLemmatizer()\n",
        "# train_data['new_tweet'] = train_data['new_tweet'].apply(lambda x : [lemma.lemmatize(i, wordnet.VERB ) for i in x] )\n",
        "# print(train_data['new_tweet'])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0        when father dysfunct selfish drag kid into dys...\n",
            "1        thank #lyft credit caus they offer wheelchair ...\n",
            "2                                      bihday your majesti\n",
            "3                               #model love take with time\n",
            "4                                 factsguid societi #motiv\n",
            "                               ...                        \n",
            "31957                                           that youuu\n",
            "31958    nina turner airwav tri wrap herself mantl genu...\n",
            "31959                         listen song monday morn work\n",
            "31960          #sikh #templ vandalis #calgari #wso condemn\n",
            "31961                                         thank follow\n",
            "Name: new_tweet, Length: 31962, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzI9nrE4IO8C",
        "outputId": "0f1a14e4-5aac-4947-d8d9-b35494e9bd7c"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count = CountVectorizer()\n",
        "frequency = count.fit_transform(train_data['new_tweet'])\n",
        "print(len(frequency.todense()))\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "Tfidf = TfidfVectorizer()\n",
        "Tfidf_freq = Tfidf.fit_transform(train_data['new_tweet'])\n",
        "print(Tfidf_freq.todense())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "31962\n",
            "[[0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " ...\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]\n",
            " [0. 0. 0. ... 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMhzz4BwMzor"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train_bow, x_valid_bow, y_train_bow, y_valid_bow = train_test_split(frequency,train_data['label'],test_size=0.3,random_state=2)\n",
        "x_train_tfidf, x_valid_tfidf, y_train_tfidf, y_valid_tfidf = train_test_split(Tfidf_freq,train_data['label'],test_size=0.3,random_state=17)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAnAE0t72ztC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e6a217d-83fd-4781-ff01-1c2a22a0f077"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "logistic_reg = LogisticRegression(random_state = 0, solver = 'lbfgs')\n",
        "logistic_reg.fit(x_train_bow, y_train_bow)\n",
        "\n",
        "prediction_bow = logistic_reg.predict_proba(x_valid_bow)\n",
        "prediction_bow\n",
        "\n",
        "prediction_int = prediction_bow[:,1]>=0.3\n",
        "\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "prediction_int\n",
        "\n",
        "log_bow = f1_score(y_valid_bow, prediction_int)\n",
        "log_bow"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6589403973509934"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ce9wFZws9784",
        "outputId": "be7ec479-47b1-4020-f306-b60b0d58c70f"
      },
      "source": [
        "logistic_reg.fit(x_train_tfidf, y_train_tfidf)\n",
        "\n",
        "prediction_tfidf = logistic_reg.predict_proba(x_valid_tfidf)\n",
        "prediction_tfidf\n",
        "\n",
        "prediction_int = prediction_tfidf[:,1]>=0.3\n",
        "predictiono_int = prediction_int.astype(np.int)\n",
        "prediction_int\n",
        "\n",
        "log_tfidf = f1_score(y_valid_tfidf, prediction_int)\n",
        "log_tfidf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6084452975047985"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IsH2pNMIMuiv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_7N0GwONSpo"
      },
      "source": [
        ""
      ]
    }
  ]
}